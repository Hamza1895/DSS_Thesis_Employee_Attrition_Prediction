{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS_9KidQ0klq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn numpy pandas imbalanced-learn optuna tab-transformer-pytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "numerical_columns = [x for x in data.select_dtypes(include=['int64', 'float64']).columns if x!= \"Attrition\"]\n",
    "\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "nominal_columns = [ 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJB71ugGUFsp",
    "outputId": "ac74c6a8-101c-4dbd-c147-26c046fb4d28",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training code follows the example: https://github.com/lucidrains/tab-transformer-pytorch/issues/6\n",
    "\n",
    "N_TRIALS = 50\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from io import StringIO\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "sampling_techniques = {\n",
    "    \"None\": None,\n",
    "    \"SMOTE\": SMOTE(random_state=42),\n",
    "    \"BorderlineSMOTE\": BorderlineSMOTE(random_state=42),\n",
    "    \"SMOTETomek\": SMOTETomek(random_state=42),\n",
    "    \"ADASYN\": ADASYN(random_state=42),\n",
    "}\n",
    "\n",
    "X_categ = data[categorical_columns]\n",
    "\n",
    "## using label encoder for tab transformer\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_columns}\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    le = label_encoders[col]\n",
    "    X_categ[col] = le.fit_transform(X_categ[col])\n",
    "\n",
    "\n",
    "X_cont = data[numerical_columns].values\n",
    "\n",
    "y = X_categ['Attrition'].values\n",
    "X_categ = X_categ.drop(columns=[\"Attrition\"], errors='ignore')\n",
    "X_categ = X_categ.values.astype(np.int64)\n",
    "\n",
    "y = y.astype(np.int64)\n",
    "X_train_categ, X_test_categ, X_train_cont, X_test_cont, y_train, y_test = train_test_split(\n",
    "    X_categ, X_cont, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "## change type to int, avoids library error\n",
    "X_test_categ = X_test_categ.astype(np.int64)\n",
    "X_test_cont = X_test_cont.astype(np.float32)\n",
    "\n",
    "## convert everything to correct tensor type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_test_categ_tensor = torch.tensor(X_test_categ, dtype=torch.long).to(device)\n",
    "X_test_cont_tensor = torch.tensor(X_test_cont, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "categorical_columns = [x for x in categorical_columns if x != \"Attrition\"]\n",
    "def objective(trial, sampling_technique):\n",
    "    dim = 32\n",
    "    depth = 6\n",
    "    heads = 8\n",
    "    dim_out = 1\n",
    "    attn_dropout = trial.suggest_float(\"attn_dropout\", 0.1, 0.5)\n",
    "    ff_dropout = trial.suggest_float(\"ff_dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight = trial.suggest_int(\"weight\", 1,5, step=1)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_categ, y_train):\n",
    "        X_train_fold_categ, X_val_fold_categ = X_train_categ[train_idx], X_train_categ[val_idx]\n",
    "        X_train_fold_cont, X_val_fold_cont = X_train_cont[train_idx], X_train_cont[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "     \n",
    "        X_train_fold_categ = X_train_fold_categ.astype(np.int64)\n",
    "        X_val_fold_categ = X_val_fold_categ.astype(np.int64)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold_cont = scaler.fit_transform(X_train_fold_cont)\n",
    "        X_val_fold_cont = scaler.transform(X_val_fold_cont)\n",
    "\n",
    "        cont_mean = X_train_fold_cont.mean(0)\n",
    "        cont_std = X_train_fold_cont.std(0)\n",
    "        cont_mean = torch.tensor(cont_mean, dtype=torch.float) if not isinstance(cont_mean, torch.Tensor) else cont_mean.float()\n",
    "        cont_std = torch.tensor(cont_std, dtype=torch.float) if not isinstance(cont_std, torch.Tensor) else cont_std.float()\n",
    "\n",
    "\n",
    "\n",
    "        cont_mean_std = torch.stack([cont_mean.to(device), cont_std.to(device)], dim=1)\n",
    "\n",
    "        if sampling_technique is not None:\n",
    "            X_train_combined = np.hstack((X_train_fold_categ, X_train_fold_cont))\n",
    "            X_resampled, y_resampled = sampling_technique.fit_resample(X_train_combined, y_train_fold)\n",
    "            X_train_fold_categ, X_train_fold_cont = X_resampled[:, :len(categorical_columns)], X_resampled[:, len(categorical_columns):]\n",
    "            y_train_fold = y_resampled\n",
    "\n",
    "        X_train_categ_tensor = torch.tensor(X_train_fold_categ, dtype=torch.long).to(device)\n",
    "        X_train_cont_tensor = torch.tensor(X_train_fold_cont, dtype=torch.float).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train_fold, dtype=torch.long).to(device)\n",
    "\n",
    "        X_val_categ_tensor = torch.tensor(X_val_fold_categ, dtype=torch.long).to(device)\n",
    "        X_val_cont_tensor = torch.tensor(X_val_fold_cont, dtype=torch.float).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val_fold, dtype=torch.long).to(device)\n",
    "\n",
    "        model = TabTransformer(\n",
    "            categories=tuple([len(label_encoders[col].classes_) for col in categorical_columns]),\n",
    "            num_continuous=len(numerical_columns),\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            attn_dropout=attn_dropout,\n",
    "            ff_dropout=ff_dropout,\n",
    "            mlp_act = nn.ReLU(),\n",
    "            continuous_mean_std = cont_mean_std\n",
    "        ).to(device)\n",
    "\n",
    "        pos_weight = torch.tensor([weight], device=device)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_train_categ_tensor, X_train_cont_tensor).squeeze(1)\n",
    "            loss = criterion(preds.to(device), y_train_tensor.float().to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_categ_tensor, X_val_cont_tensor).squeeze(1)\n",
    "            val_preds_binary = (torch.sigmoid(val_preds) > 0.5).long()\n",
    "            f1 = f1_score(y_val_tensor.cpu().numpy(), val_preds_binary.cpu().numpy(), average=\"weighted\")\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "combined_results = []\n",
    "for sampling_name, sampling_technique in sampling_techniques.items():\n",
    "    print(f\"Optimizing {sampling_name}\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, sampling_technique), n_trials=N_TRIALS)\n",
    "    try:\n",
    "        importance = get_param_importances(study)\n",
    "    except:\n",
    "        importance = None\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    if sampling_technique is not None:\n",
    "        X_train_combined = np.hstack((X_train_categ, X_train_cont))\n",
    "        X_resampled, y_resampled = sampling_technique.fit_resample(X_train_combined, y_train)\n",
    "        X_train_categ_final = X_resampled[:, :len(categorical_columns)]\n",
    "        X_train_cont_final = X_resampled[:, len(categorical_columns):]\n",
    "    else:\n",
    "        X_train_categ_final, X_train_cont_final, y_resampled = X_train_categ, X_train_cont, y_train\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cont_final = scaler.fit_transform(X_train_cont_final)\n",
    "    X_test_cont_scaled = scaler.transform(X_test_cont)\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train_categ_tensor = torch.tensor(X_train_categ_final.astype(np.int64), dtype=torch.long).to(device)\n",
    "    X_train_cont_tensor = torch.tensor(X_train_cont_final.astype(np.float32), dtype=torch.float).to(device)\n",
    "    y_train_tensor = torch.tensor(y_resampled, dtype=torch.long).to(device)\n",
    "\n",
    "    cont_mean = X_train_cont_tensor.mean(0)\n",
    "    cont_std = X_train_cont_tensor.std(0)\n",
    "    cont_mean = torch.tensor(cont_mean) if not isinstance(cont_mean, torch.Tensor) else cont_mean\n",
    "    cont_std = torch.tensor(cont_std) if not isinstance(cont_std, torch.Tensor) else cont_std\n",
    "    cont_mean_std = torch.stack([cont_mean, cont_std], dim=1)\n",
    "\n",
    "    best_model = TabTransformer(\n",
    "        categories=tuple([len(label_encoders[col].classes_) for col in categorical_columns]),\n",
    "        num_continuous=len(numerical_columns),\n",
    "        dim=32,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        attn_dropout=best_params[\"attn_dropout\"],\n",
    "        ff_dropout=best_params[\"ff_dropout\"],\n",
    "        mlp_act = nn.ReLU(),\n",
    "        continuous_mean_std = cont_mean_std\n",
    "    ).to(device)\n",
    "\n",
    "    # tried with pos weight as well but it didn't improve anything\n",
    "    pos_weight = torch.tensor([best_params[\"weight\"]], device=device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "    best_model.train()\n",
    "    for epoch in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        preds = best_model(X_train_categ_tensor, X_train_cont_tensor).squeeze(1)\n",
    "        loss = criterion(preds, y_train_tensor.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = best_model(X_test_categ_tensor, X_test_cont_tensor).squeeze(1)\n",
    "        test_preds_prob = torch.sigmoid(test_preds).cpu().numpy()\n",
    "        test_preds_binary = (test_preds_prob > 0.5).astype(int)\n",
    "\n",
    "    f1_weighted = f1_score(y_test, test_preds_binary, average=\"weighted\")\n",
    "    precision_weighted = precision_score(y_test, test_preds_binary, average=\"weighted\")\n",
    "    recall_weighted = recall_score(y_test, test_preds_binary, average=\"weighted\")\n",
    "    roc_auc = roc_auc_score(y_test, test_preds_prob)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_preds_prob, pos_label=1)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    conf_matrix = confusion_matrix(y_test, test_preds_binary)   \n",
    "    accuracy = (test_preds_binary == y_test).mean()\n",
    "\n",
    "    combined_results.append({\n",
    "        \"Model\": \"TabTransformer\",\n",
    "        \"Sampling\": sampling_name,\n",
    "        \"Validation F1-Score (CV)\": study.best_value,\n",
    "        \"Test F1-Score (Weighted)\": f1_weighted,\n",
    "        \"Test Precision (Weighted)\": precision_weighted,\n",
    "        \"Test Recall (Weighted)\": recall_weighted,\n",
    "        \"Test ROC-AUC\": roc_auc,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Confusion Matrix\": conf_matrix.tolist(),\n",
    "        \"Best Parameters\": best_params,\n",
    "        \"importance\": importance\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuZBMFWq6W1J",
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(combined_results)\n",
    "results_df.to_csv(\"tabtransformer_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
