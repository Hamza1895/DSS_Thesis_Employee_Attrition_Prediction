{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS_9KidQ0klq"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pytorch-tabnet scikit-learn numpy pandas imbalanced-learn optuna --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "numerical_columns = [x for x in data.select_dtypes(include=['int64', 'float64']).columns if x!= \"Attrition\"]\n",
    "\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "processed_data = data.copy()\n",
    "\n",
    "nominal_columns = [ 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "processed_data = pd.get_dummies(processed_data, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "processed_data['Attrition'] = processed_data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "processed_data['OverTime'] = processed_data['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "processed_data['Gender'] = processed_data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "processed_data['BusinessTravel'] = processed_data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})\n",
    "\n",
    "X_processed = processed_data.drop(columns=[\"Attrition\"], errors='ignore').values\n",
    "y_processed = processed_data[\"Attrition\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 22:41:59,061] A new study created in memory with name: no-name-ef104633-acf1-496e-9bb8-baf702bc88c0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.02189 | validation_f1_score: 0.74294 |  0:00:16s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_validation_f1_score = 0.74294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.5653  | validation_f1_score: 0.76533 |  0:00:17s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_validation_f1_score = 0.76533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.7966  | validation_f1_score: 0.76456 |  0:00:20s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_validation_f1_score = 0.76456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.75552 | validation_f1_score: 0.66689 |  0:00:18s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_validation_f1_score = 0.66689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.05711 | validation_f1_score: 0.7466  |  0:00:18s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_validation_f1_score = 0.7466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-12-06 22:44:08,651] Trial 0 finished with value: 0.737263078674314 and parameters: {'n_d': 49, 'n_a': 61, 'n_steps': 9, 'n_independent': 2, 'n_shared': 4, 'gamma': 1.46820430309878, 'lambda_sparse': 0.007277558878295638, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.737263078674314.\n",
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.74798 | test_f1_score: 0.76376 |  0:00:23s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_test_f1_score = 0.76376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=1\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from optuna.importance import get_param_importances\n",
    "import torch \n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "\n",
    "sampling_techniques = {\n",
    "    \"None\": None,\n",
    "    \"SMOTE\": SMOTE(random_state=42),\n",
    "    \"BorderlineSMOTE\": BorderlineSMOTE(random_state=42),\n",
    "    \"SMOTETomek\": SMOTETomek(random_state=42),\n",
    "    \"ADASYN\": ADASYN(random_state=42),\n",
    "}\n",
    "\n",
    "class F1WeightedMetric(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1_score\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        y_score = np.where(y_score > 0.5, 1, 0)\n",
    "        y_score = y_score[:,1]\n",
    "        return f1_score(y_true, y_score, average=\"weighted\")\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial, sampling_technique):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 2, 10),\n",
    "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 4),\n",
    "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 4),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-2),\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adadelta\", \"Adam\", \"Adagrad\", \"Adamax\", \"RMSprop\", \"SGD\"]),\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_cv, X_val_cv = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_cv, y_val_cv = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        if sampling_technique is not None:\n",
    "            X_train_cv_resampled, y_train_cv_resampled = sampling_technique.fit_resample(X_train_cv, y_train_cv)\n",
    "        else:\n",
    "            X_train_cv_resampled, y_train_cv_resampled = X_train_cv, y_train_cv\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_cv_resampled = scaler.fit_transform(X_train_cv_resampled)\n",
    "        X_val_cv = scaler.transform(X_val_cv)\n",
    "\n",
    "        tabnet_model = TabNetClassifier(\n",
    "            n_d=params[\"n_d\"],\n",
    "            n_a=params[\"n_a\"],\n",
    "            n_steps=params[\"n_steps\"],\n",
    "            gamma=params[\"gamma\"],\n",
    "            lambda_sparse=params[\"lambda_sparse\"],\n",
    "            n_independent=params[\"n_independent\"],\n",
    "            n_shared=params[\"n_shared\"],\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "            scheduler_params = {\"gamma\": params[\"gamma\"],\n",
    "                     \"step_size\": 20},\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "             momentum=0.3, clip_value=2., cat_emb_dim=1,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        tabnet_model.fit(\n",
    "            X_train_cv_resampled, y_train_cv_resampled,\n",
    "            eval_set=[(X_val_cv, y_val_cv)],\n",
    "            eval_name=[\"validation\"],\n",
    "            eval_metric=[F1WeightedMetric],\n",
    "            max_epochs=1,\n",
    "            batch_size=2,\n",
    "            patience=10,\n",
    "\n",
    "        )\n",
    "\n",
    "        y_val_pred = tabnet_model.predict(X_val_cv)\n",
    "        f1_weighted = f1_score(y_val_cv, y_val_pred, average=\"weighted\")\n",
    "        f1_scores.append(f1_weighted)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for sampling_name, sampling_technique in sampling_techniques.items():\n",
    "    print(f\"Optimizing {sampling_name}\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, sampling_technique), n_trials=N_TRIALS)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    try:\n",
    "        importance = get_param_importances(study)\n",
    "    except:\n",
    "        importance = None\n",
    "\n",
    "    if sampling_technique is not None:\n",
    "        X_train_resampled, y_train_resampled = sampling_technique.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)  \n",
    "    \n",
    "    best_model = TabNetClassifier(\n",
    "        n_d=best_params[\"n_d\"],\n",
    "        n_a=best_params[\"n_a\"],\n",
    "        n_steps=best_params[\"n_steps\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        lambda_sparse=best_params[\"lambda_sparse\"],\n",
    "        n_independent=best_params[\"n_independent\"],\n",
    "        n_shared=best_params[\"n_shared\"],\n",
    "        optimizer_fn=getattr(torch.optim, best_params[\"optimizer\"]),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(\n",
    "        X_train_resampled, y_train_resampled,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        eval_name=[\"test\"],\n",
    "        eval_metric=[F1WeightedMetric],\n",
    "        max_epochs=1,\n",
    "        batch_size=2,\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "    y_test_pred = best_model.predict(X_test_scaled)\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    f1_weighted = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    precision_weighted = precision_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    recall_weighted = recall_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    combined_results.append({\n",
    "            \"Model\": \"TabNet\",\n",
    "            \"Sampling\": sampling_name,\n",
    "            \"Validation F1-Score (CV)\": study.best_trial.value,\n",
    "            \"Test F1-Score (Weighted)\": f1_weighted,\n",
    "            \"Test Precision (Weighted)\": precision_weighted,\n",
    "            \"Test Recall (Weighted)\": recall_weighted,\n",
    "            \"Test ROC-AUC\": roc_auc,\n",
    "            \"Test PR-AUC\": pr_auc,\n",
    "            \"Best Parameters\": best_params,\n",
    "            \"hyperparameter_importance\": importance,\n",
    "            \"confusion_matrix\": conf_matrix\n",
    "        })\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(combined_results)\n",
    "results_df.to_csv(\"tabnet_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
