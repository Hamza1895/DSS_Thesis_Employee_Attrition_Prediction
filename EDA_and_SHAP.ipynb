{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2710a8-d142-4a43-a990-6bd9d1e96880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hvplot.pandas\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import io\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import shap\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "pd.set_option(\"display.max_rows\", 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a58a47-45c9-4e74-889f-81f7f1ff2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IBM Dataset 1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3baf0-8f52-48b5-9fc8-81722647ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd76f93-ac5a-45e9-85fb-6c93630f0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "#'Age', 'JobLevel', 'StockOptionLevel', 'YearsWithCurrManager', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'YearsAtCompany', 'PerformanceRating', 'PercentSalaryhike' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541484c5-5de0-43d7-8bbc-bbacec9751a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Imbalance bar chart\n",
    "\n",
    "\n",
    "df = pd.read_csv('IBM Dataset 1.csv')\n",
    "df['Attrition'] = df['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "attrition_counts = df['Attrition'].value_counts()\n",
    "labels = ['No', 'Yes']\n",
    "counts = [attrition_counts[0], attrition_counts[1]]\n",
    "\n",
    "total = sum(counts)\n",
    "percentages = [count / total * 100 for count in counts]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(labels, counts, color=['lightblue', 'lightcoral'], alpha=0.7, label=labels)\n",
    "\n",
    "for bar, count, percentage in zip(bars, counts, percentages):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, f'{count} ({percentage:.1f}%)',\n",
    "            ha='center', va='center', fontsize=14, fontweight='bold', color='black')\n",
    "\n",
    "ax.set_xlabel('Attrition Outcome', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Employee Attrition: Count and Percentage', fontsize=14)\n",
    "ax.legend(bars, ['No Attrition', 'Attrition'], loc='upper right', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('attrition_bar_chart.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662e4e3-9df3-4acf-b840-7f38618d00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features correlated with 'Attrition'\n",
    "\n",
    "df = pd.read_csv('IBM Dataset 1.csv')\n",
    "\n",
    "df['Attrition'] = df['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)\n",
    "col = df.corr(numeric_only=True).Attrition.index\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df[col].corr(), annot=True, cmap='seismic', annot_kws={'size': 12}, center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features Correlated with Attrition', fontsize=20)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout(pad=2)\n",
    "#plt.savefig('correlation_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b45b9-3069-45cb-829a-d09251eae7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 correlations for numerical variabels with Attrition\n",
    "\n",
    "df = pd.read_csv('IBM Dataset 1.csv')\n",
    "\n",
    "df['Attrition'] = df['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)\n",
    "\n",
    "correlation_matrix = df.corr(numeric_only=True)\n",
    "attrition_corr = correlation_matrix['Attrition'].drop('Attrition')\n",
    "\n",
    "print(\"Correlation values with Attrition:\\n\", attrition_corr)\n",
    "\n",
    "sorted_attrition_corr = attrition_corr.abs().sort_values(ascending=False).head(10)\n",
    "top_features = sorted_attrition_corr.index\n",
    "correlations = sorted_attrition_corr.values \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "plt.gca().set_facecolor('white')\n",
    "sns.set_style(\"whitegrid\") \n",
    "sns.barplot(x=correlations, y=top_features, palette='pastel', orient='h')\n",
    "plt.xlabel('Absolute Correlation with Attrition', loc='center', color='black')\n",
    "plt.ylabel('Features', loc='center', color='black')\n",
    "plt.title('Top 10 Features based on Correlation with Employee Attrition', loc='left', color='black')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('top_features_attrition.png', facecolor='white', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb61f3-2419-4dde-b0b4-ff879b47b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    n = contingency_table.sum().sum()  # Total number of observations\n",
    "    cramers_v_value = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "    \n",
    "    return cramers_v_value\n",
    "\n",
    "categorical_variables = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "\n",
    "cramers_v_results = []\n",
    "for column in categorical_variables:\n",
    "    cramers_v_value = cramers_v(df['Attrition'], df[column])\n",
    "    cramers_v_results.append({'Variable': column, \"Cramér's V\": cramers_v_value})\n",
    "\n",
    "\n",
    "cramers_v_df = pd.DataFrame(cramers_v_results)\n",
    "cramers_v_df = cramers_v_df.sort_values(by=\"Cramér's V\", ascending=False)\n",
    "print(cramers_v_df)\n",
    "\n",
    "#cramers_v_df.to_csv('cramers_v_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ee488-9d1b-4d23-84f6-a6b7a6b43291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create bins dynamically, group data, and plot bar charts with counts\n",
    "\n",
    "def plot_numerical_attrition_with_duplicates_handling(data, column, bins=None, bin_labels=None):\n",
    "    \"\"\"\n",
    "    This function creates bins for continuous numerical columns (if applicable), groups data by Attrition,\n",
    "    plots bar charts, and adds counts on the bins with correct legend using colored boxes.\n",
    "    Handles duplicate bin edges gracefully.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        column (str): The numerical column to analyze.\n",
    "        bins (list, optional): List of bin edges for continuous columns. If None, bins are generated dynamically.\n",
    "        bin_labels (list, optional): List of labels for the bins. If None, labels are created dynamically.\n",
    "\n",
    "    Returns:\n",
    "        Displays a bar chart and corresponding table.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(data[column].unique()) > 10:\n",
    "        if bins is None:\n",
    "            bins = pd.qcut(data[column], q=10, duplicates='drop', retbins=True)[1]  # Deciles with duplicate handling\n",
    "            bin_labels = [f\"{int(bins[i])}-{int(bins[i+1])}\" for i in range(len(bins)-1)]\n",
    "        \n",
    "        bin_column = pd.cut(data[column], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "        data[f\"{column}Bins\"] = bin_column\n",
    "        group_column = f\"{column}Bins\"\n",
    "    else:\n",
    "        \n",
    "        group_column = column\n",
    "\n",
    "    grouped_data = data.groupby([group_column, 'Attrition']).size().unstack(fill_value=0)\n",
    "\n",
    "    grouped_data['Total'] = grouped_data.sum(axis=1)\n",
    "    grouped_data['% Attrition in Cluster'] = round((grouped_data['Yes'] / grouped_data['Total']) * 100, 1)\n",
    "    grouped_data['% of Total Attrition'] = round((grouped_data['Yes'] / grouped_data['Yes'].sum()) * 100, 1)\n",
    "\n",
    "    grouped_data = grouped_data.reset_index()\n",
    "\n",
    "    plot_data = grouped_data.melt(\n",
    "        id_vars=[group_column], \n",
    "        value_vars=['No', 'Yes'], \n",
    "        var_name='Attrition', \n",
    "        value_name='Count'\n",
    "    )\n",
    "\n",
    "    custom_palette = ['lightblue', 'lightcoral']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=plot_data, x=group_column, y='Count', hue='Attrition', dodge=True, palette=custom_palette)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=9, color='black')\n",
    "\n",
    "    plt.title(f'Attrition Distribution by {column}')\n",
    "    plt.xlabel(f'{column}' if group_column == f\"{column}Bins\" else column)\n",
    "    plt.ylabel('Number of Employees')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = ['No Attrition', 'Attrition']\n",
    "    ax.legend(handles, labels, title='Attrition', loc='upper right', frameon=True)\n",
    "\n",
    "    #plt.savefig(f'attrition_distribution_by_{column}.png', format='png', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    grouped_data.to_csv(f'attrition_distribution_values_by_{column}.csv')\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "    \n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset and removes unnecessary columns.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    columns_to_drop = ['EmployeeNumber', 'StandardHours', 'Over18', 'EmployeeCount']\n",
    "    data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    return data\n",
    "\n",
    "file_path =  'IBM Dataset 1.csv'\n",
    "cleaned_data = load_and_prepare_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac4938-b421-443a-b382-dbc74e775298",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cleaned_data.columns:\n",
    "    if col == \"Attrition\":\n",
    "        continue\n",
    "    plot_numerical_attrition_with_duplicates_handling(cleaned_data, column=col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea12ffc-7342-47a2-a642-f74e94c8cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numerical_attrition_with_duplicates_handling(cleaned_data, column=\"Gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09d507-7c52-4288-9d5b-8f0531ef1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-Step Feature Importance (SHAP) Analysis using XGBoost\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "nominal_columns = ['Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "data = pd.get_dummies(data, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "data['Attrition'] = data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "data['OverTime'] = data['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "data['BusinessTravel'] = data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})\n",
    "\n",
    "X_processed = data.drop(columns=['Attrition'], errors='ignore')\n",
    "y_processed = data['Attrition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=189,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.4214,\n",
    "    scale_pos_weight=1,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "explainer_xgb = shap.Explainer(xgb_model, X_train_smote)\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46895dcd-3166-43e0-a6ed-9c99fc8627c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-Step Feature Importance Analysis using MLP (SHAP)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "nominal_columns = ['Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "data = pd.get_dummies(data, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "data['Attrition'] = data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "data['OverTime'] = data['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "data['BusinessTravel'] = data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})\n",
    "\n",
    "X_processed = data.drop(columns=['Attrition'], errors='ignore')\n",
    "y_processed = data['Attrition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64, 32, 16),\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    alpha=1.7442758481025684e-05,\n",
    "    learning_rate_init=0.0008722872705873338,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "explainer_mlp = shap.Explainer(mlp_model.predict, X_train_smote)\n",
    "shap_values_mlp = explainer_mlp(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c010666-814d-4ab0-b028-ca24e528c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Analysis SHAP using TabNet\n",
    "\n",
    "class F1WeightedMetric(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1_score\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        y_score = np.where(y_score > 0.5, 1, 0)\n",
    "        y_score = y_score[:,1]\n",
    "        return f1_score(y_true, y_score, average=\"weighted\")\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "data = pd.read_csv(\"IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "nominal_columns = ['Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "data = pd.get_dummies(data, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "data['Attrition'] = data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "data['OverTime'] = data['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "data['BusinessTravel'] = data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})\n",
    "\n",
    "X_processed = data.drop(columns=['Attrition'], errors='ignore')\n",
    "y_processed = data['Attrition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = borderline_smote.fit_resample(X_train, y_train)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=62, \n",
    "    n_a=41, \n",
    "    n_steps=2,\n",
    "    gamma=1.2628907527283806, \n",
    "    lambda_sparse=0.003385618571289165,\n",
    "    optimizer_fn=torch.optim.RMSprop,\n",
    "    optimizer_params=dict(lr=2e-3),\n",
    "    device_name=device,\n",
    "    n_independent=4,\n",
    "    n_shared=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_train_smote.values, y_train_smote.values,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=64,\n",
    "    \n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = tabnet_model.predict(X_test_np)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
