{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS_9KidQ0klq"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn numpy pandas imbalanced-learn optuna lightgbm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zieMRdKnQnXf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/IBM Dataset 1.csv\")\n",
    "\n",
    "data = data.drop(columns=['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], errors='ignore')\n",
    "\n",
    "engineered_features = [\n",
    "    ('IncomePerJobLevel', lambda df: df['MonthlyIncome'] / (df['JobLevel'] + 1)),\n",
    "    ('TotalWorkingYearsToJobLevelRatio', lambda df: df['TotalWorkingYears'] / (df['JobLevel'] + 1)),\n",
    "    ('YearsAtCompanyToAgeRatio', lambda df: df['YearsAtCompany'] / (df['Age'] + 1)),\n",
    "    ('YearsAtCompanyToYearsInCurrentRoleRatio', lambda df: df['YearsAtCompany'] / (df['YearsInCurrentRole'] + 1))\n",
    "]\n",
    "\n",
    "for name, func in engineered_features:\n",
    "    data[name] = func(data)\n",
    "\n",
    "numerical_columns = [x for x in data.select_dtypes(include=['int64', 'float64']).columns if x!= \"Attrition\"]\n",
    "\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "processed_data = data.copy()\n",
    "\n",
    "nominal_columns = [ 'Department', 'EducationField', 'JobRole', 'MaritalStatus']\n",
    "processed_data = pd.get_dummies(processed_data, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "processed_data['Attrition'] = processed_data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "processed_data['OverTime'] = processed_data['OverTime'].map({'No': 0, 'Yes': 1})\n",
    "processed_data['Gender'] = processed_data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "processed_data['BusinessTravel'] = processed_data['BusinessTravel'].map({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})\n",
    "\n",
    "X_processed = processed_data.drop(columns=[\"Attrition\"], errors='ignore')\n",
    "y_processed = processed_data[\"Attrition\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all ML models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moDniTdYsnC2",
    "outputId": "dfede715-f83c-49ac-82a2-f8f9997d6df2"
   },
   "outputs": [],
   "source": [
    "N_TRIALS = 2\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from optuna.importance import get_param_importances\n",
    "import lightgbm as lgb\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "\n",
    "\n",
    "sampling_techniques = {\n",
    "    \"None\": None,\n",
    "    \"SMOTE\": SMOTE(random_state=42),\n",
    "    \"BorderlineSMOTE\": BorderlineSMOTE(random_state=42),\n",
    "    \"SMOTETomek\": SMOTETomek(random_state=42),\n",
    "    \"ADASYN\": ADASYN(random_state=42),\n",
    "}\n",
    "\n",
    "models_and_params = {\n",
    "    \"Logistic Regression\": LogisticRegression,\n",
    "    \"KNN\": KNeighborsClassifier,\n",
    "    #\"SVM\": SVC,\n",
    "    \"Decision Tree\": DecisionTreeClassifier,\n",
    "    \"Random Forest\": RandomForestClassifier,\n",
    "    \"AdaBoost\": AdaBoostClassifier,\n",
    "    \"XGBoost\": XGBClassifier,\n",
    "    \"Gaussian Naive Bayes\": GaussianNB,\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier,\n",
    "    \"LightGBM\": lgb.LGBMClassifier\n",
    "}\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for sampling_name, sampler_type in sampling_techniques.items():\n",
    "    for model_name, model_class in models_and_params.items():\n",
    "        print(f\"Optimizing {model_name}\")\n",
    "\n",
    "        def objective(trial, sampler):\n",
    "            if model_name == \"Logistic Regression\":\n",
    "                model = LogisticRegression(\n",
    "                    C=trial.suggest_float(\"C\", 1e-3, 1e3, log=True),\n",
    "                    penalty=trial.suggest_categorical(\"penalty\", [\"l2\"]),\n",
    "                    class_weight=trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "                    max_iter=trial.suggest_categorical(\"max_iter\", [1000]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"KNN\":\n",
    "                model = KNeighborsClassifier(\n",
    "                    n_neighbors=trial.suggest_int(\"n_neighbors\", 3, 15),\n",
    "                    weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "                    metric=trial.suggest_categorical(\"metric\", [\"euclidean\", \"manhattan\", \"minkowski\"])\n",
    "                )\n",
    "            elif model_name == \"SVM\":\n",
    "                model = SVC(\n",
    "                    C=trial.suggest_categorical(\"C\", [0.001, 0.01, 0.1, 1, 10, 100, 1000]),\n",
    "                    gamma=trial.suggest_categorical(\"gamma\", [\"scale\", 0.01, 0.1, 1]),\n",
    "                    kernel=trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"]),\n",
    "                    probability=trial.suggest_categorical(\"probability\", [True]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"Decision Tree\":\n",
    "                model = DecisionTreeClassifier(\n",
    "                    max_depth=trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                    min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                    class_weight=trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "                    criterion=trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"Random Forest\":\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "                    max_depth=trial.suggest_int(\"max_depth\", 5, 15),\n",
    "                    min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                    min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "                    class_weight=trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"AdaBoost\":\n",
    "                model = AdaBoostClassifier(\n",
    "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1, log=True),\n",
    "                    algorithm=trial.suggest_categorical(\"algorithm\", [\"SAMME\"]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"XGBoost\":\n",
    "                model = XGBClassifier(\n",
    "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "                    max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1, log=True),\n",
    "                    scale_pos_weight=trial.suggest_categorical(\"scale_pos_weight\", [1, 10, 50]),\n",
    "                    eval_metric=trial.suggest_categorical(\"eval_metric\", [\"logloss\"]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"Gaussian Naive Bayes\":\n",
    "                model = GaussianNB()\n",
    "            elif model_name == \"Gradient Boosting\":\n",
    "                model = GradientBoostingClassifier(\n",
    "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1, log=True),\n",
    "                    max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    subsample=trial.suggest_categorical(\"subsample\", [0.8, 1.0]),\n",
    "                    random_state=42\n",
    "                )\n",
    "            elif model_name == \"LightGBM\":\n",
    "                model = lgb.LGBMClassifier(\n",
    "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1.0, log=True), \n",
    "                    max_depth=trial.suggest_int(\"max_depth\", -1, 15), \n",
    "                    num_leaves=trial.suggest_int(\"num_leaves\", 2, 64),\n",
    "                    feature_fraction=trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "                    bagging_fraction=trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "                    bagging_freq=trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "                    min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "                    lambda_l1=trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "                    lambda_l2=trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            f1_scores = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "                X_train_cv, X_val_cv = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                preprocessor = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('num', numeric_transformer, numerical_columns)],\n",
    "                    remainder='passthrough')\n",
    "\n",
    "                if sampler:\n",
    "\n",
    "                    X_train_cv, y_train_cv = sampler.fit_resample(X_train_cv, y_train_cv)\n",
    "                    X_train_cv = preprocessor.fit_transform(X_train_cv)\n",
    "                else:\n",
    "                    X_train_cv = preprocessor.fit_transform(X_train_cv)\n",
    "                \n",
    "                X_val_cv = preprocessor.transform(X_val_cv)\n",
    "\n",
    "                model.fit(X_train_cv, y_train_cv)\n",
    "                y_pred_cv = model.predict(X_val_cv)\n",
    "\n",
    "                f1 = f1_score(y_val_cv, y_pred_cv, average=\"weighted\")\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "                trial.report(f1, fold)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            return np.mean(f1_scores)\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(lambda trial: objective(trial, sampler_type), n_trials=N_TRIALS)\n",
    "\n",
    "        try:\n",
    "            importance = get_param_importances(study)\n",
    "        except:\n",
    "            importance = None\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "\n",
    "        model_params = best_params.copy()\n",
    "        if model_name in [\"KNN\", \"Gaussian Naive Bayes\"]:\n",
    "            best_model = model_class(**model_params)\n",
    "        else:\n",
    "            best_model = model_class(**model_params, random_state=42)\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numerical_columns)],remainder='passthrough')\n",
    "        if sampler_type:\n",
    "            X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "            X_train_resampled, y_train_resampled = sampler_type.fit_resample(X_train_transformed, y_train)\n",
    "        else:\n",
    "            X_train_resampled = preprocessor.fit_transform(X_train)\n",
    "            y_train_resampled = y_train\n",
    "\n",
    "\n",
    "        best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "        y_pred_test = best_model.predict(X_test_transformed)\n",
    "        y_prob_test = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "        f1_weighted = f1_score(y_test, y_pred_test, average=\"weighted\")\n",
    "        precision_weighted = precision_score(y_test, y_pred_test, average=\"weighted\")\n",
    "        recall_weighted = recall_score(y_test, y_pred_test, average=\"weighted\")\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "     \n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob_test)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        combined_results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Sampling\": sampling_name,\n",
    "            \"Validation F1-Score (CV)\": best_trial.value,\n",
    "            \"Test F1-Score (Weighted)\": f1_weighted,\n",
    "            \"Test Precision (Weighted)\": precision_weighted,\n",
    "            \"Test Recall (Weighted)\": recall_weighted,\n",
    "            \"Test ROC-AUC\": roc_auc,\n",
    "            \"Test PR-AUC\": pr_auc,\n",
    "            \"Best Parameters\": best_params,\n",
    "            \"hyperparameter_importance\": importance,\n",
    "            \"confusion_matrix\": conf_matrix\n",
    "        })\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(combined_results)\n",
    "\n",
    "results_df.to_csv(\"ml_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
